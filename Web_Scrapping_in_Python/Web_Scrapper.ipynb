{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7001fcd93f074860687356353637fa8a9bcfae26351191961070d25d38eb8654"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Web Scrapper\n",
    "Let’s start by grabbing all the HTML code from a single web page. You’ll use a page on Real Python that’s been set up for use with this tutorial.\n",
    "\n",
    "One useful package for web scraping that you can find in Python’s standard library is __urllib__, which contains tools for working with __URLs__. In particular, the __urllib.request__ module contains a function called __urlopen()__ that can be used to open a __URL__ within a program."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "print(dir(urlopen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x1de63f35c08>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "# To open the web page, pass url to urlopen():\n",
    "page = urlopen(url)\n",
    "\n",
    "# urlopen() returns an HTTPResponse object:\n",
    "page"
   ]
  },
  {
   "source": [
    "To extract the HTML from the page, first use the __HTTPResponse__ object’s __.read()__ method, which returns a sequence of bytes. Then use .decode() to decode the bytes to a string using UTF-8:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<html>\n<head>\n<title>Profile: Aphrodite</title>\n</head>\n<body bgcolor=\"yellow\">\n<center>\n<br><br>\n<img src=\"/static/aphrodite.gif\" />\n<h2>Name: Aphrodite</h2>\n<br><br>\nFavorite animal: Dove\n<br><br>\nFavorite color: Red\n<br><br>\nHometown: Mount Olympus\n</center>\n</body>\n</html>\n\n"
     ]
    }
   ],
   "source": [
    "html_bytes = page.read()\n",
    "html = html_bytes.decode('utf-8')\n",
    "print(html)"
   ]
  },
  {
   "source": [
    "Once you have the HTML as text, you can extract information from it in a couple of different ways.\n",
    "\n",
    "### Extract Text From HTML With String Methods\n",
    "One way to extract information from a web page’s HTML is to use string methods. For instance, you can use __.find()__ to search through the text of the HTML for the <title> tags and extract the title of the web page.\n",
    "\n",
    "Let’s extract the __title__ of the web page you requested in the previous example. If you know the index of the first character of the title and the first character of the closing </title> tag, then you can use a string slice to extract the title.\n",
    "\n",
    "Since __.find()__ returns the index of the first occurrence of a substring, you can get the index of the opening __<title>__ tag by passing the string \"<title>\" to __.find()__:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "title_index = html.find('<title>')\n",
    "title_index"
   ]
  },
  {
   "source": [
    "You don’t want the index of the <title> tag, though. You want the index of the title itself. To get the index of the first letter in the title, you can add the length of the string \"<title>\" to title_index:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "start_index = title_index + len(\"<title>\")\n",
    "start_index"
   ]
  },
  {
   "source": [
    "Now get the index of the closing </title> tag by passing the string \"</title>\" to .find():"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "end_index = html.find(\"</title>\")\n",
    "end_index"
   ]
  },
  {
   "source": [
    "Finally, you can extract the title by slicing the html string:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Profile: Aphrodite'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "title = html[start_index:end_index]\n",
    "title"
   ]
  },
  {
   "source": [
    "### Regular Expressions\n",
    "__Regular expressions__ —or __regexes__ for short—are patterns that can be used to search for text within a string. Python supports regular expressions through the standard library’s __re__ module."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['A', 'ASCII', 'DEBUG', 'DOTALL', 'I', 'IGNORECASE', 'L', 'LOCALE', 'M', 'MULTILINE', 'Match', 'Pattern', 'RegexFlag', 'S', 'Scanner', 'T', 'TEMPLATE', 'U', 'UNICODE', 'VERBOSE', 'X', '_MAXCACHE', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', '_cache', '_compile', '_compile_repl', '_expand', '_locale', '_pickle', '_special_chars_map', '_subx', 'compile', 'copyreg', 'enum', 'error', 'escape', 'findall', 'finditer', 'fullmatch', 'functools', 'match', 'purge', 'search', 'split', 'sre_compile', 'sre_parse', 'sub', 'subn', 'template']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(dir(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__Regular expressions__ use special characters called __metacharacters__ to denote different patterns. For instance, the asterisk character (*) stands for zero or more of whatever comes just before the asterisk.\n",
    "\n",
    "In the following example, you use __findall()__ to find any text within a string that matches a given regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ac']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"ac\")"
   ]
  },
  {
   "source": [
    "The first argument of __re.findall()___ is the regular expression that you want to match, and the second argument is the string to test. In the above example, you search for the pattern \"ab*c\" in the string \"ac\".\n",
    "\n",
    "The regular expression \"ab*c\" matches any part of the string that begins with an \"a\", ends with a \"c\", and has zero or more instances of \"b\" between the two. __re.findall()__ returns a list of all matches. The string \"ac\" matches this pattern, so it’s returned in the list.\n",
    "\n",
    "Here’s the same pattern applied to different strings:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abc']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"abcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ac']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abc', 'ac']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"abcac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"abdc\")"
   ]
  },
  {
   "source": [
    "Notice that if no match is found, then findall() returns an empty list.\n",
    "\n",
    "Pattern matching is case sensitive. If you want to match this pattern regardless of the case, then you can pass a third argument with the value __re.IGNORECASE__:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"ABC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ABC']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "re.findall(\"ab*c\", \"ABC\", re.IGNORECASE)"
   ]
  },
  {
   "source": [
    "You can use a period (.) to stand for any single character in a regular expression. For instance, you could find all the strings that contain the letters \"a\" and \"c\" separated by a single character as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abc']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "re.findall(\"a.c\", \"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "re.findall(\"a.c\", \"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "re.findall(\"a.c\", \"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['acc']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "re.findall(\"a.c\", \"acc\")"
   ]
  },
  {
   "source": [
    "The pattern __.*__ inside a regular expression stands for any character repeated any number of times. For instance, \"a.*c\" can be used to find every substring that starts with \"a\" and ends with \"c\", regardless of which letter—or letters—are in between:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abc']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "re.findall(\"a.*c\", \"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abbc']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "re.findall(\"a.*c\", \"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ac']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "re.findall(\"a.*c\", \"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['acc']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "re.findall(\"a.*c\", \"acc\")"
   ]
  },
  {
   "source": [
    "Often, you use __re.search()__ to search for a particular pattern inside a string. This function is somewhat more complicated than __re.findall()__ because it returns an object called a __MatchObject__ that stores different groups of data. This is because there might be matches inside other matches, and __re.search()__ returns every possible result.\n",
    "\n",
    "calling __.group()__ on a __MatchObject__ will return the first and most inclusive result, which in most cases is just what you want:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ABC'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "match_results = re.search(\"ab*c\", \"ABC\", re.IGNORECASE)\n",
    "match_results.group()"
   ]
  },
  {
   "source": [
    "There’s one more function in the re module that’s useful for parsing out text. __re.sub()__, which is short for __substitute__, allows you to replace text in a string that matches a regular expression with new text. It behaves sort of like the __.replace()__ string method.\n",
    "\n",
    "The arguments passed to __re.sub()__ are the regular expression, followed by the replacement text, followed by the string. Here’s an example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Everything is ELEPHANTS.'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "source": [
    "Perhaps that wasn’t quite what you expected to happen.\n",
    "\n",
    "__re.sub()__ uses the regular expression \"<.*>\" to find and replace everything between the first < and last >, which spans from the beginning of <replaced> to the end of <tags>. This is because Python’s regular expressions are __greedy__, meaning they try to find the longest possible match when characters like * are used.\n",
    "\n",
    "Alternatively, you can use the non-greedy matching pattern __*?__, which works the same way as * except that it matches the shortest possible string of text:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Everything is ELEPHANTS if it's in ELEPHANTS.\""
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*?>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "source": [
    "This time, __re.sub()__ finds two matches, <replaced> and <tags>, and substitutes the string \"ELEPHANTS\" for both matches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Extract Text From HTML With Regular Expressions\n",
    "let’s now try to parse out the title from a new profile page (http://olympus.realpython.org/profiles/dionysus), which includes this rather carelessly written line of HTML:\n",
    "\n",
    "The __.find()__ method would have a difficult time dealing with the inconsistencies here, but with the clever use of regular expressions, you can handle this code quickly and efficiently:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Profile: Dionysus\n"
     ]
    }
   ],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "title = re.sub(\"<.*?>\", \"\", title) # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "source": [
    "## Use an HTML Parser for Web Scraping in Python\n",
    "Although regular expressions are great for pattern matching in general, sometimes it’s easier to use an HTML parser that’s explicitly designed for parsing out HTML pages. There are many Python tools written for this purpose, but the __Beautiful Soup__ library is a good one to start with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Create a BeautifulSoup Object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "source": [
    "This program does three things:\n",
    "\n",
    "1. Opens the URL http://olympus.realpython.org/profiles/dionysus using urlopen() from the urllib.request module\n",
    "2. Reads the HTML from the page as a string and assigns it to the html variable\n",
    "3. Creates a BeautifulSoup object and assigns it to the soup variable\n",
    "\n",
    "The __BeautifulSoup__ object assigned to soup is created with two arguments. The first argument is the HTML to be parsed, and the second argument, the string \"html.parser\", tells the object which parser to use behind the scenes. \"html.parser\" represents Python’s built-in HTML parser."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Use a BeautifulSoup Object\n",
    "Save and run the above program. When it’s finished running, you can use the soup variable in the interactive window to parse the content of html in various ways.\n",
    "\n",
    "For example, __BeautifulSoup__ objects have a __.get_text()__ method that can be used to extract all the text from the document and automatically remove any HTML tags."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nProfile: Dionysus\n\n\n\n\n\nName: Dionysus\n\nHometown: Mount Olympus\n\nFavorite animal: Leopard \n\nFavorite Color: Wine\n\n\n\n\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "source": [
    "There are a lot of blank lines in this output. These are the result of newline characters in the HTML document’s text. You can remove them with the string __.replace()__ method if you need to.\n",
    "\n",
    "Often, you need to get only specific text from an HTML document. Using Beautiful Soup first to extract the text and then using the __.find()__ string method is sometimes easier than working with regular expressions.\n",
    "\n",
    "However, sometimes the HTML tags themselves are the elements that point out the data you want to retrieve. For instance, perhaps you want to retrieve the URLs for all the images on the page. These links are contained in the src attribute of <img> HTML tags.\n",
    "\n",
    "In this case, you can use find_all() to return a list of all instances of that particular tag:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>, <img src=\"/static/grapes.png\"/>]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "soup.find_all(\"img\")"
   ]
  },
  {
   "source": [
    "This returns a list of all <img> tags in the HTML document. The objects in the list look like they might be strings representing the tags, but they’re actually instances of the Tag object provided by Beautiful Soup. Tag objects provide a simple interface for working with the information they contain.\n",
    "\n",
    "Let’s explore this a little by first unpacking the Tag objects from the list:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, image2 = soup.find_all(\"img\")"
   ]
  },
  {
   "source": [
    "Each Tag object has a .name property that returns a string containing the HTML tag type:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'img'"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "image1.name"
   ]
  },
  {
   "source": [
    "You can access the HTML attributes of the Tag object by putting their name between square brackets, just as if the attributes were keys in a dictionary.\n",
    "\n",
    "For example, the <img src=\"/static/dionysus.jpg\"/> tag has a single attribute, src, with the value \"/static/dionysus.jpg\". Likewise, an HTML tag such as the link <a href=\"https://realpython.com\" target=\"_blank\"> has two attributes, href and target.\n",
    "\n",
    "To get the source of the images in the Dionysus profile page, you access the src attribute using the dictionary notation mentioned above:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/static/dionysus.jpg'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "image1[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/static/grapes.png'"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "image2[\"src\"]"
   ]
  },
  {
   "source": [
    "Certain tags in HTML documents can be accessed by properties of the Tag object. For example, to get the <title> tag in a document, you can use the .title property:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<title>Profile: Dionysus</title>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "source": [
    "Beautiful Soup automatically cleans up the tags for you by removing the extra space in the opening tag and the extraneous forward slash (/) in the closing tag.\n",
    "\n",
    "You can also retrieve just the string between the title tags with the .string property of the Tag object:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Profile: Dionysus'"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "source": [
    "One of the more useful features of __Beautiful Soup__ is the ability to search for specific kinds of tags whose attributes match certain values. For example, if you want to find all the <img> tags that have a src attribute equal to the value /static/dionysus.jpg, then you can provide the following additional argument to .find_all():"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "soup.find_all(\"img\", src=\"/static/dionysus.jpg\")"
   ]
  },
  {
   "source": [
    "When scraping data from websites with Python, you’re often interested in particular parts of the page. By spending some time looking through the HTML document, you can identify tags with unique attributes that you can use to extract the data you need.\n",
    "\n",
    "Then, instead of relying on complicated regular expressions or using .find() to search through the document, you can directly access the particular tag you’re interested in and extract the data you need."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "BeautifulSoup is great for scraping data from a website’s HTML, but it doesn’t provide any way to work with HTML forms. For example, if you need to search a website for some query and then scrape the results, then BeautifulSoup alone won’t get you very far."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Interact With HTML Forms\n",
    "The __urllib__ module you’ve been working with so far in this tutorial is well suited for requesting the contents of a web page. Sometimes, though, you need to interact with a web page to obtain the content you need. For example, you might need to submit a form or click a button to display hidden content.\n",
    "\n",
    "The Python standard library doesn’t provide a built-in means for working with web pages interactively, but many third-party packages are available from PyPI. Among these, __MechanicalSoup__ is a popular and relatively straightforward package to use.\n",
    "\n",
    "In essence, MechanicalSoup installs what’s known as a headless browser, which is a web browser with no graphical user interface. This browser is controlled programmatically via a Python program."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```py\n",
    "pip install mechanicalsoup\n",
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### References\n",
    "1. https://realpython.com/python-web-scraping-practical-introduction/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}